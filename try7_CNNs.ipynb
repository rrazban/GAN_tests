{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "Our seventh attempt to improve default results from coursera_lab.ipynb:\n",
    "- change the GAN training protocol by directly handling derivates using tf.GradientTape(), (sixth attempt)\n",
    "- have the encoder and decoder models be composed of Convoluted Neural Networks\n",
    "- inspired by Google tutorial (https://www.tensorflow.org/tutorials/generative/dcgan)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-step instructions \n",
    "\n",
    "### Step 1: Data preprocessing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 14:50:52.571166: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-16 14:50:52.627117: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install tensorflow-cpu==2.16.2\n",
    "\n",
    "# Suppress warnings and set environment variables\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import warnings\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "# Suppress all Python warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to the range [-1, 1]\n",
    "x_train = x_train.astype('float32') / 127.5 - 1.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Building the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,266,944</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,176</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">819,328</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,864</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,601</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │     \u001b[38;5;34m1,266,944\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │        \u001b[38;5;34m50,176\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m819,328\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │       \u001b[38;5;34m204,864\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │         \u001b[38;5;34m1,601\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,343,681</span> (8.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,343,681\u001b[0m (8.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,318,209</span> (8.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,318,209\u001b[0m (8.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,472</span> (99.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m25,472\u001b[0m (99.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6272</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,273</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m1,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6272\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m6,273\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">212,865</span> (831.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m212,865\u001b[0m (831.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">212,865</span> (831.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m212,865\u001b[0m (831.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, Model \n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape, Flatten, Input, Conv2D, Conv2DTranspose, Reshape, MaxPooling2D, Dropout, LeakyReLU\n",
    "\n",
    "factor = 2\n",
    "\n",
    "# Define the generator model \n",
    "def build_generator(): \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(7*7*256, input_dim=100)) \n",
    "    model.add(BatchNormalization(momentum=0.8)) \n",
    "    model.add(LeakyReLU(alpha=0.2)) \n",
    "\n",
    "    model.add(Reshape((7, 7, 256)))\n",
    "    model.add(Conv2DTranspose(64*factor, (5, 5), strides=(1, 1), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=0.8)) \n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2DTranspose(32*factor, (5, 5), strides=(2, 2), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=0.8)) \n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', activation='tanh')) #final image needs to be 28*28*1, so filter=1 and strides is (2*2) to multiply dimension by two from previous layer\n",
    "\n",
    "    return model \n",
    "\n",
    "# Define the discriminator model \n",
    "def build_discriminator(): \n",
    "    model = Sequential() \n",
    "\n",
    "    model.add(Conv2D(32*factor, (5, 5), strides=(2,2), padding='same', input_shape=[28, 28, 1]))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(64*factor, (5, 5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "    return model \n",
    "\n",
    "\n",
    "# Build and compile the generator \n",
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator() \n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "\n",
    "generator.summary()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training the GAN \n",
    "\n",
    "#### Objective: \n",
    "- Train the GAN on the MNIST dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.3888044357299805]  [G loss: 0.6788032054901123]\n",
      "10 [D loss: 1.053548812866211]  [G loss: 0.6266166567802429]\n",
      "20 [D loss: 0.9881212711334229]  [G loss: 0.547370970249176]\n",
      "30 [D loss: 1.036490559577942]  [G loss: 0.4977784752845764]\n",
      "40 [D loss: 1.0704014301300049]  [G loss: 0.5153522491455078]\n",
      "50 [D loss: 1.0599493980407715]  [G loss: 0.6683135032653809]\n",
      "60 [D loss: 1.3195412158966064]  [G loss: 0.498264878988266]\n",
      "70 [D loss: 1.3037443161010742]  [G loss: 0.5619645118713379]\n",
      "80 [D loss: 1.083861231803894]  [G loss: 0.7127848863601685]\n",
      "90 [D loss: 1.0412795543670654]  [G loss: 0.6752016544342041]\n",
      "100 [D loss: 1.015824794769287]  [G loss: 0.7291425466537476]\n",
      "110 [D loss: 1.0784504413604736]  [G loss: 0.7127937078475952]\n",
      "120 [D loss: 0.9948174953460693]  [G loss: 0.8335347771644592]\n",
      "130 [D loss: 0.8181041479110718]  [G loss: 0.958892822265625]\n",
      "140 [D loss: 1.3757598400115967]  [G loss: 0.5458407998085022]\n",
      "150 [D loss: 1.120511531829834]  [G loss: 0.7271530628204346]\n",
      "160 [D loss: 1.2067153453826904]  [G loss: 0.7894116640090942]\n",
      "170 [D loss: 1.3955118656158447]  [G loss: 0.6851047277450562]\n",
      "180 [D loss: 1.5398015975952148]  [G loss: 0.5989177227020264]\n",
      "190 [D loss: 1.4695568084716797]  [G loss: 0.6222679615020752]\n",
      "200 [D loss: 1.4396169185638428]  [G loss: 0.6556894183158875]\n",
      "210 [D loss: 1.553159236907959]  [G loss: 0.5629363059997559]\n",
      "220 [D loss: 1.648628830909729]  [G loss: 0.5679329633712769]\n",
      "230 [D loss: 1.504881739616394]  [G loss: 0.5820755958557129]\n",
      "240 [D loss: 1.3094974756240845]  [G loss: 0.7118690013885498]\n",
      "250 [D loss: 1.2040843963623047]  [G loss: 0.7692962884902954]\n",
      "260 [D loss: 1.1794631481170654]  [G loss: 0.7989296317100525]\n",
      "270 [D loss: 1.3424689769744873]  [G loss: 0.6979584693908691]\n",
      "280 [D loss: 1.6709249019622803]  [G loss: 0.6007700562477112]\n",
      "290 [D loss: 1.523073673248291]  [G loss: 0.6093332767486572]\n",
      "300 [D loss: 1.26762056350708]  [G loss: 0.6881663203239441]\n",
      "310 [D loss: 1.150517463684082]  [G loss: 0.7954579591751099]\n",
      "320 [D loss: 1.138303279876709]  [G loss: 0.7744781970977783]\n",
      "330 [D loss: 1.1608855724334717]  [G loss: 0.7626574635505676]\n",
      "340 [D loss: 1.2714159488677979]  [G loss: 0.7027225494384766]\n",
      "350 [D loss: 1.3325200080871582]  [G loss: 0.6963419914245605]\n",
      "360 [D loss: 1.318252682685852]  [G loss: 0.7210664749145508]\n",
      "370 [D loss: 1.2272052764892578]  [G loss: 0.7381982803344727]\n",
      "380 [D loss: 1.3215187788009644]  [G loss: 0.7235226631164551]\n",
      "390 [D loss: 1.4742391109466553]  [G loss: 0.6691023111343384]\n",
      "400 [D loss: 1.378883719444275]  [G loss: 0.7390273809432983]\n",
      "410 [D loss: 1.102330207824707]  [G loss: 0.8608422875404358]\n",
      "420 [D loss: 1.0136072635650635]  [G loss: 0.9040444493293762]\n",
      "430 [D loss: 0.9706000089645386]  [G loss: 1.0355485677719116]\n",
      "440 [D loss: 0.7488416433334351]  [G loss: 1.1966829299926758]\n",
      "450 [D loss: 0.6231831312179565]  [G loss: 1.376022458076477]\n",
      "460 [D loss: 0.8837950229644775]  [G loss: 1.1032817363739014]\n",
      "470 [D loss: 1.5807148218154907]  [G loss: 0.6816615462303162]\n",
      "480 [D loss: 1.3373913764953613]  [G loss: 0.6768667101860046]\n",
      "490 [D loss: 1.0270367860794067]  [G loss: 0.8647340536117554]\n",
      "500 [D loss: 0.7351309061050415]  [G loss: 1.0650408267974854]\n",
      "510 [D loss: 0.7494648098945618]  [G loss: 1.0648843050003052]\n",
      "520 [D loss: 1.2893702983856201]  [G loss: 0.8835084438323975]\n",
      "530 [D loss: 1.0439648628234863]  [G loss: 1.1147263050079346]\n",
      "540 [D loss: 0.6798677444458008]  [G loss: 1.4331978559494019]\n",
      "550 [D loss: 0.601325511932373]  [G loss: 1.3738083839416504]\n",
      "560 [D loss: 0.6284754276275635]  [G loss: 1.279350757598877]\n",
      "570 [D loss: 0.6933959722518921]  [G loss: 1.2621662616729736]\n",
      "580 [D loss: 0.8316276669502258]  [G loss: 1.1732807159423828]\n",
      "590 [D loss: 0.9851242899894714]  [G loss: 1.1294972896575928]\n",
      "600 [D loss: 0.5452328324317932]  [G loss: 1.2771334648132324]\n",
      "610 [D loss: 0.44003036618232727]  [G loss: 1.6044511795043945]\n",
      "620 [D loss: 0.9165534973144531]  [G loss: 1.2400844097137451]\n",
      "630 [D loss: 1.1427900791168213]  [G loss: 1.032728910446167]\n",
      "640 [D loss: 0.7145804762840271]  [G loss: 1.3750700950622559]\n",
      "650 [D loss: 0.5344686508178711]  [G loss: 1.6036920547485352]\n",
      "660 [D loss: 0.5938869714736938]  [G loss: 1.4667444229125977]\n",
      "670 [D loss: 0.649494469165802]  [G loss: 1.4294805526733398]\n",
      "680 [D loss: 1.2297008037567139]  [G loss: 1.0185649394989014]\n",
      "690 [D loss: 1.4768524169921875]  [G loss: 0.7374446392059326]\n",
      "700 [D loss: 0.9884829521179199]  [G loss: 1.12591552734375]\n",
      "710 [D loss: 0.6370079517364502]  [G loss: 1.3211544752120972]\n",
      "720 [D loss: 0.58766770362854]  [G loss: 1.3705265522003174]\n",
      "730 [D loss: 0.8299421072006226]  [G loss: 1.4885917901992798]\n",
      "740 [D loss: 0.7570723295211792]  [G loss: 1.1393651962280273]\n",
      "750 [D loss: 0.953197717666626]  [G loss: 1.453696846961975]\n",
      "760 [D loss: 0.62774258852005]  [G loss: 1.3899366855621338]\n",
      "770 [D loss: 0.6484634876251221]  [G loss: 1.406301736831665]\n",
      "780 [D loss: 0.802979588508606]  [G loss: 1.3599015474319458]\n",
      "790 [D loss: 1.1320762634277344]  [G loss: 1.2147691249847412]\n",
      "800 [D loss: 1.240500807762146]  [G loss: 0.9393636584281921]\n",
      "810 [D loss: 0.9662727117538452]  [G loss: 1.2090909481048584]\n",
      "820 [D loss: 0.7124313116073608]  [G loss: 1.6115137338638306]\n",
      "830 [D loss: 0.7447712421417236]  [G loss: 1.2973294258117676]\n",
      "840 [D loss: 1.455873966217041]  [G loss: 1.002089023590088]\n",
      "850 [D loss: 1.6413593292236328]  [G loss: 0.7632067799568176]\n",
      "860 [D loss: 1.5320158004760742]  [G loss: 0.9531004428863525]\n",
      "870 [D loss: 1.2134981155395508]  [G loss: 0.9717806577682495]\n",
      "880 [D loss: 1.277951955795288]  [G loss: 1.2218528985977173]\n",
      "890 [D loss: 1.6996803283691406]  [G loss: 0.6500285863876343]\n",
      "900 [D loss: 1.4854940176010132]  [G loss: 0.7490083575248718]\n",
      "910 [D loss: 1.2802810668945312]  [G loss: 1.2934789657592773]\n",
      "920 [D loss: 1.198913335800171]  [G loss: 0.9830674529075623]\n",
      "930 [D loss: 1.10996675491333]  [G loss: 0.9776180982589722]\n",
      "940 [D loss: 1.2125282287597656]  [G loss: 0.9927443265914917]\n",
      "950 [D loss: 1.3687303066253662]  [G loss: 0.8883278369903564]\n",
      "960 [D loss: 1.2118151187896729]  [G loss: 0.812859058380127]\n",
      "970 [D loss: 1.0631005764007568]  [G loss: 1.221763253211975]\n",
      "980 [D loss: 0.9059402942657471]  [G loss: 1.161303162574768]\n",
      "990 [D loss: 1.0435367822647095]  [G loss: 0.8968396186828613]\n",
      "1000 [D loss: 1.0982459783554077]  [G loss: 1.0363664627075195]\n",
      "1010 [D loss: 1.4108350276947021]  [G loss: 0.7821564674377441]\n",
      "1020 [D loss: 1.443105697631836]  [G loss: 0.7369072437286377]\n",
      "1030 [D loss: 1.1302292346954346]  [G loss: 1.264084815979004]\n",
      "1040 [D loss: 0.7850794792175293]  [G loss: 1.4037264585494995]\n",
      "1050 [D loss: 0.6193802356719971]  [G loss: 1.3720459938049316]\n",
      "1060 [D loss: 0.6812173128128052]  [G loss: 1.4896951913833618]\n",
      "1070 [D loss: 0.8361935615539551]  [G loss: 1.5232363939285278]\n",
      "1080 [D loss: 0.8629736304283142]  [G loss: 1.2527337074279785]\n",
      "1090 [D loss: 1.2647570371627808]  [G loss: 1.5155344009399414]\n",
      "1100 [D loss: 1.7487366199493408]  [G loss: 0.8221623301506042]\n",
      "1110 [D loss: 1.183911919593811]  [G loss: 0.8299556970596313]\n",
      "1120 [D loss: 0.7638354301452637]  [G loss: 1.1475918292999268]\n",
      "1130 [D loss: 1.3095424175262451]  [G loss: 0.8443434238433838]\n",
      "1140 [D loss: 1.1859909296035767]  [G loss: 1.2902755737304688]\n",
      "1150 [D loss: 0.7767554521560669]  [G loss: 1.2783193588256836]\n",
      "1160 [D loss: 1.0839447975158691]  [G loss: 0.9346588850021362]\n",
      "1170 [D loss: 2.2967424392700195]  [G loss: 0.45948928594589233]\n",
      "1180 [D loss: 1.3555352687835693]  [G loss: 0.7029880285263062]\n",
      "1190 [D loss: 0.8502517342567444]  [G loss: 1.36495840549469]\n",
      "1200 [D loss: 1.0342025756835938]  [G loss: 1.059680700302124]\n",
      "1210 [D loss: 1.8503429889678955]  [G loss: 0.8287717700004578]\n",
      "1220 [D loss: 1.616860032081604]  [G loss: 0.8834757804870605]\n",
      "1230 [D loss: 1.208287239074707]  [G loss: 1.041861653327942]\n",
      "1240 [D loss: 0.9482317566871643]  [G loss: 1.2237709760665894]\n",
      "1250 [D loss: 0.767448902130127]  [G loss: 1.2124077081680298]\n",
      "1260 [D loss: 0.9426641464233398]  [G loss: 1.162071943283081]\n",
      "1270 [D loss: 1.136602759361267]  [G loss: 0.9228810667991638]\n",
      "1280 [D loss: 1.059561014175415]  [G loss: 0.9048137664794922]\n",
      "1290 [D loss: 0.9357962608337402]  [G loss: 1.1961579322814941]\n",
      "1300 [D loss: 1.0266072750091553]  [G loss: 1.281365156173706]\n",
      "1310 [D loss: 1.2627712488174438]  [G loss: 1.028294324874878]\n",
      "1320 [D loss: 1.3126475811004639]  [G loss: 0.9789106845855713]\n",
      "1330 [D loss: 1.3542449474334717]  [G loss: 0.9460157155990601]\n",
      "1340 [D loss: 1.1484079360961914]  [G loss: 0.9691391587257385]\n",
      "1350 [D loss: 1.245002269744873]  [G loss: 0.9594883918762207]\n",
      "1360 [D loss: 1.231475830078125]  [G loss: 1.0227253437042236]\n",
      "1370 [D loss: 1.2488802671432495]  [G loss: 0.9965139627456665]\n",
      "1380 [D loss: 1.5562163591384888]  [G loss: 0.9946802854537964]\n",
      "1390 [D loss: 2.1453897953033447]  [G loss: 0.5677428245544434]\n",
      "1400 [D loss: 1.64902663230896]  [G loss: 0.6948909759521484]\n",
      "1410 [D loss: 1.6544264554977417]  [G loss: 0.7998852729797363]\n",
      "1420 [D loss: 1.4565794467926025]  [G loss: 0.7105962038040161]\n",
      "1430 [D loss: 1.236164927482605]  [G loss: 0.79731684923172]\n",
      "1440 [D loss: 1.174018383026123]  [G loss: 0.915290355682373]\n",
      "1450 [D loss: 1.2874081134796143]  [G loss: 0.9517483115196228]\n",
      "1460 [D loss: 1.26934814453125]  [G loss: 0.9209836721420288]\n",
      "1470 [D loss: 1.2051030397415161]  [G loss: 0.9615885019302368]\n",
      "1480 [D loss: 1.2105822563171387]  [G loss: 0.8852078914642334]\n",
      "1490 [D loss: 1.0583186149597168]  [G loss: 0.848250150680542]\n",
      "1500 [D loss: 1.23940110206604]  [G loss: 0.859652042388916]\n",
      "1510 [D loss: 1.1450884342193604]  [G loss: 1.050561547279358]\n",
      "1520 [D loss: 1.2148466110229492]  [G loss: 0.9113849401473999]\n",
      "1530 [D loss: 0.9831464290618896]  [G loss: 1.034120798110962]\n",
      "1540 [D loss: 0.9585448503494263]  [G loss: 1.20448637008667]\n",
      "1550 [D loss: 1.2081489562988281]  [G loss: 1.0300533771514893]\n",
      "1560 [D loss: 1.2191979885101318]  [G loss: 0.9290077686309814]\n",
      "1570 [D loss: 1.2979015111923218]  [G loss: 0.8904802799224854]\n",
      "1580 [D loss: 1.2934350967407227]  [G loss: 0.9340053200721741]\n",
      "1590 [D loss: 1.1250851154327393]  [G loss: 1.0639976263046265]\n",
      "1600 [D loss: 1.065760850906372]  [G loss: 1.1559062004089355]\n",
      "1610 [D loss: 1.358633279800415]  [G loss: 0.7792248725891113]\n",
      "1620 [D loss: 1.5548779964447021]  [G loss: 0.6035463809967041]\n",
      "1630 [D loss: 1.1848669052124023]  [G loss: 0.7598673701286316]\n",
      "1640 [D loss: 0.8169266581535339]  [G loss: 1.1844748258590698]\n",
      "1650 [D loss: 0.8204894065856934]  [G loss: 1.3025232553482056]\n",
      "1660 [D loss: 1.0030453205108643]  [G loss: 1.2298269271850586]\n",
      "1670 [D loss: 1.2676223516464233]  [G loss: 0.8770745992660522]\n",
      "1680 [D loss: 1.3361119031906128]  [G loss: 0.8742306232452393]\n",
      "1690 [D loss: 1.3209950923919678]  [G loss: 0.9403324723243713]\n",
      "1700 [D loss: 1.432271957397461]  [G loss: 0.7520186305046082]\n",
      "1710 [D loss: 1.414730191230774]  [G loss: 0.6498029232025146]\n",
      "1720 [D loss: 1.288341760635376]  [G loss: 0.7948004603385925]\n",
      "1730 [D loss: 1.1155614852905273]  [G loss: 0.9570159316062927]\n",
      "1740 [D loss: 1.026123046875]  [G loss: 1.154862880706787]\n",
      "1750 [D loss: 0.9468637108802795]  [G loss: 1.1031919717788696]\n",
      "1760 [D loss: 1.0426173210144043]  [G loss: 1.0499942302703857]\n",
      "1770 [D loss: 1.328691840171814]  [G loss: 0.9668774604797363]\n",
      "1780 [D loss: 1.463453769683838]  [G loss: 0.9915896058082581]\n",
      "1790 [D loss: 1.3142168521881104]  [G loss: 0.8960496187210083]\n",
      "1800 [D loss: 1.246932029724121]  [G loss: 1.0200833082199097]\n",
      "1810 [D loss: 1.2830891609191895]  [G loss: 1.0535376071929932]\n",
      "1820 [D loss: 1.1631817817687988]  [G loss: 1.0966954231262207]\n",
      "1830 [D loss: 1.3191583156585693]  [G loss: 0.8485113382339478]\n",
      "1840 [D loss: 1.388139009475708]  [G loss: 0.8253011703491211]\n",
      "1850 [D loss: 1.3994653224945068]  [G loss: 0.7132874727249146]\n",
      "1860 [D loss: 1.650455117225647]  [G loss: 0.758857250213623]\n",
      "1870 [D loss: 1.6251072883605957]  [G loss: 0.7863591909408569]\n",
      "1880 [D loss: 1.3382866382598877]  [G loss: 0.8743343353271484]\n",
      "1890 [D loss: 1.2629709243774414]  [G loss: 0.8224776983261108]\n",
      "1900 [D loss: 1.3238205909729004]  [G loss: 0.7855615019798279]\n",
      "1910 [D loss: 1.2360236644744873]  [G loss: 0.941696047782898]\n",
      "1920 [D loss: 1.4019598960876465]  [G loss: 0.9382962584495544]\n",
      "1930 [D loss: 1.2855654954910278]  [G loss: 0.9297511577606201]\n",
      "1940 [D loss: 1.4574484825134277]  [G loss: 0.860033392906189]\n",
      "1950 [D loss: 1.358731746673584]  [G loss: 0.7992625832557678]\n",
      "1960 [D loss: 1.3124998807907104]  [G loss: 0.8085393309593201]\n",
      "1970 [D loss: 1.241682529449463]  [G loss: 0.8444564342498779]\n",
      "1980 [D loss: 1.337357521057129]  [G loss: 0.9336583018302917]\n",
      "1990 [D loss: 1.254441738128662]  [G loss: 0.8722999691963196]\n",
      "2000 [D loss: 1.415708303451538]  [G loss: 0.8750789761543274]\n",
      "2010 [D loss: 1.4691768884658813]  [G loss: 0.8654755353927612]\n",
      "2020 [D loss: 1.179038166999817]  [G loss: 1.009178876876831]\n",
      "2030 [D loss: 1.0574655532836914]  [G loss: 1.0617129802703857]\n",
      "2040 [D loss: 0.9942007064819336]  [G loss: 1.1988012790679932]\n",
      "2050 [D loss: 1.2863104343414307]  [G loss: 1.0640522241592407]\n",
      "2060 [D loss: 1.3179149627685547]  [G loss: 0.8811192512512207]\n",
      "2070 [D loss: 1.1503996849060059]  [G loss: 0.9280934929847717]\n",
      "2080 [D loss: 1.0294108390808105]  [G loss: 1.0494142770767212]\n",
      "2090 [D loss: 1.075303077697754]  [G loss: 1.0877286195755005]\n",
      "2100 [D loss: 1.1324599981307983]  [G loss: 1.0555062294006348]\n",
      "2110 [D loss: 1.084299087524414]  [G loss: 1.0080420970916748]\n",
      "2120 [D loss: 1.16364586353302]  [G loss: 1.0065813064575195]\n",
      "2130 [D loss: 1.4508484601974487]  [G loss: 0.8202800750732422]\n",
      "2140 [D loss: 1.679879069328308]  [G loss: 0.6442161202430725]\n",
      "2150 [D loss: 1.7797636985778809]  [G loss: 0.6037815809249878]\n",
      "2160 [D loss: 1.4810041189193726]  [G loss: 0.7562370300292969]\n",
      "2170 [D loss: 1.2092194557189941]  [G loss: 0.9019418954849243]\n",
      "2180 [D loss: 1.1022984981536865]  [G loss: 1.0212550163269043]\n",
      "2190 [D loss: 1.0663130283355713]  [G loss: 1.1482495069503784]\n",
      "2200 [D loss: 1.1819124221801758]  [G loss: 1.0692850351333618]\n",
      "2210 [D loss: 1.0377798080444336]  [G loss: 1.0673426389694214]\n",
      "2220 [D loss: 0.833389401435852]  [G loss: 1.145320177078247]\n",
      "2230 [D loss: 0.8688844442367554]  [G loss: 1.182823657989502]\n",
      "2240 [D loss: 0.8921398520469666]  [G loss: 1.35860276222229]\n",
      "2250 [D loss: 0.9572098255157471]  [G loss: 1.3671880960464478]\n",
      "2260 [D loss: 1.1469604969024658]  [G loss: 1.0561919212341309]\n",
      "2270 [D loss: 1.435882806777954]  [G loss: 0.7441000938415527]\n",
      "2280 [D loss: 1.157757043838501]  [G loss: 0.7941777110099792]\n",
      "2290 [D loss: 0.7128450274467468]  [G loss: 1.2050607204437256]\n",
      "2300 [D loss: 1.389602541923523]  [G loss: 1.0148402452468872]\n",
      "2310 [D loss: 2.07151460647583]  [G loss: 0.709983229637146]\n",
      "2320 [D loss: 1.7206422090530396]  [G loss: 0.7476835250854492]\n",
      "2330 [D loss: 1.3765506744384766]  [G loss: 0.851652204990387]\n",
      "2340 [D loss: 0.8964214324951172]  [G loss: 1.2269279956817627]\n",
      "2350 [D loss: 0.6601864695549011]  [G loss: 1.473732829093933]\n",
      "2360 [D loss: 0.6425597667694092]  [G loss: 1.4001379013061523]\n",
      "2370 [D loss: 0.7780685424804688]  [G loss: 1.1791528463363647]\n",
      "2380 [D loss: 1.0465213060379028]  [G loss: 0.953046441078186]\n",
      "2390 [D loss: 1.2165011167526245]  [G loss: 0.7493143677711487]\n",
      "2400 [D loss: 1.14490807056427]  [G loss: 0.8615826368331909]\n",
      "2410 [D loss: 1.038313627243042]  [G loss: 1.0017833709716797]\n",
      "2420 [D loss: 1.3932654857635498]  [G loss: 0.8240315318107605]\n",
      "2430 [D loss: 1.6756154298782349]  [G loss: 0.7548322677612305]\n",
      "2440 [D loss: 1.5745515823364258]  [G loss: 0.8052350282669067]\n",
      "2450 [D loss: 1.397128701210022]  [G loss: 0.8357772827148438]\n",
      "2460 [D loss: 1.3122737407684326]  [G loss: 0.8512928485870361]\n",
      "2470 [D loss: 1.2650697231292725]  [G loss: 0.8676496744155884]\n",
      "2480 [D loss: 1.3288342952728271]  [G loss: 0.9015504717826843]\n",
      "2490 [D loss: 1.5295631885528564]  [G loss: 0.7366613745689392]\n",
      "2500 [D loss: 1.626446008682251]  [G loss: 0.7248965501785278]\n",
      "2510 [D loss: 1.7289077043533325]  [G loss: 0.6804922819137573]\n",
      "2520 [D loss: 1.4873261451721191]  [G loss: 0.8492723703384399]\n",
      "2530 [D loss: 1.1733572483062744]  [G loss: 0.9063111543655396]\n",
      "2540 [D loss: 1.1352837085723877]  [G loss: 0.8791545033454895]\n",
      "2550 [D loss: 1.3708786964416504]  [G loss: 0.8481090664863586]\n",
      "2560 [D loss: 1.2525479793548584]  [G loss: 0.8009843826293945]\n",
      "2570 [D loss: 1.3382813930511475]  [G loss: 0.8209314942359924]\n",
      "2580 [D loss: 1.3357986211776733]  [G loss: 0.8775433301925659]\n",
      "2590 [D loss: 1.3761496543884277]  [G loss: 0.9031621217727661]\n",
      "2600 [D loss: 1.3059470653533936]  [G loss: 0.8719678521156311]\n",
      "2610 [D loss: 1.301113247871399]  [G loss: 0.859117865562439]\n",
      "2620 [D loss: 1.1776831150054932]  [G loss: 0.9065601825714111]\n",
      "2630 [D loss: 1.182752251625061]  [G loss: 0.8009531497955322]\n",
      "2640 [D loss: 1.2531979084014893]  [G loss: 0.8501337766647339]\n",
      "2650 [D loss: 1.2826223373413086]  [G loss: 0.8331187963485718]\n",
      "2660 [D loss: 1.3550517559051514]  [G loss: 0.8908712863922119]\n",
      "2670 [D loss: 1.3251500129699707]  [G loss: 0.852636456489563]\n",
      "2680 [D loss: 1.175517201423645]  [G loss: 0.9768928289413452]\n",
      "2690 [D loss: 1.0651576519012451]  [G loss: 0.9861027002334595]\n",
      "2700 [D loss: 1.2035340070724487]  [G loss: 0.8606394529342651]\n",
      "2710 [D loss: 1.3450957536697388]  [G loss: 0.7824180126190186]\n",
      "2720 [D loss: 1.3817417621612549]  [G loss: 0.9228020310401917]\n",
      "2730 [D loss: 1.4368928670883179]  [G loss: 0.8704378604888916]\n",
      "2740 [D loss: 1.3309075832366943]  [G loss: 0.7984322905540466]\n",
      "2750 [D loss: 1.436574101448059]  [G loss: 0.8475354909896851]\n",
      "2760 [D loss: 1.4853794574737549]  [G loss: 0.863253653049469]\n",
      "2770 [D loss: 1.324878215789795]  [G loss: 0.7157196402549744]\n",
      "2780 [D loss: 1.223848819732666]  [G loss: 0.8513205647468567]\n",
      "2790 [D loss: 1.1698293685913086]  [G loss: 0.9137160778045654]\n",
      "2800 [D loss: 1.1196928024291992]  [G loss: 0.9513334035873413]\n",
      "2810 [D loss: 1.1832343339920044]  [G loss: 0.9338398575782776]\n",
      "2820 [D loss: 1.2364099025726318]  [G loss: 0.8688186407089233]\n",
      "2830 [D loss: 1.2407206296920776]  [G loss: 0.90207839012146]\n",
      "2840 [D loss: 1.1117497682571411]  [G loss: 0.9673734903335571]\n",
      "2850 [D loss: 1.1026839017868042]  [G loss: 1.0852446556091309]\n",
      "2860 [D loss: 1.0698106288909912]  [G loss: 1.0274288654327393]\n",
      "2870 [D loss: 1.130491018295288]  [G loss: 1.0184699296951294]\n",
      "2880 [D loss: 1.4287638664245605]  [G loss: 0.8071105480194092]\n",
      "2890 [D loss: 1.5683190822601318]  [G loss: 0.7256656885147095]\n",
      "2900 [D loss: 1.4593100547790527]  [G loss: 0.7030170559883118]\n",
      "2910 [D loss: 1.2793811559677124]  [G loss: 0.7964837551116943]\n",
      "2920 [D loss: 1.1176122426986694]  [G loss: 0.8988078832626343]\n",
      "2930 [D loss: 1.1516485214233398]  [G loss: 0.896967351436615]\n",
      "2940 [D loss: 1.1425546407699585]  [G loss: 0.939383864402771]\n",
      "2950 [D loss: 1.20524001121521]  [G loss: 0.9586639404296875]\n",
      "2960 [D loss: 1.1580246686935425]  [G loss: 0.9868209362030029]\n",
      "2970 [D loss: 1.2011821269989014]  [G loss: 0.9342076778411865]\n",
      "2980 [D loss: 1.4134235382080078]  [G loss: 0.7563316822052002]\n",
      "2990 [D loss: 1.4601504802703857]  [G loss: 0.7041623592376709]\n",
      "3000 [D loss: 1.5358623266220093]  [G loss: 0.6976844668388367]\n",
      "3010 [D loss: 1.0426058769226074]  [G loss: 0.9885239005088806]\n",
      "3020 [D loss: 0.9660475254058838]  [G loss: 1.1136013269424438]\n",
      "3030 [D loss: 1.1383259296417236]  [G loss: 0.9805740118026733]\n",
      "3040 [D loss: 1.4159088134765625]  [G loss: 0.8554584980010986]\n",
      "3050 [D loss: 1.4279093742370605]  [G loss: 0.8520476818084717]\n",
      "3060 [D loss: 1.4304540157318115]  [G loss: 0.8501948118209839]\n",
      "3070 [D loss: 1.3030502796173096]  [G loss: 0.8234802484512329]\n",
      "3080 [D loss: 1.2719926834106445]  [G loss: 0.8067256212234497]\n",
      "3090 [D loss: 1.2398463487625122]  [G loss: 0.9026446342468262]\n",
      "3100 [D loss: 1.1620945930480957]  [G loss: 0.8331012725830078]\n",
      "3110 [D loss: 1.1658284664154053]  [G loss: 0.8231967687606812]\n",
      "3120 [D loss: 1.1589583158493042]  [G loss: 0.8580836057662964]\n",
      "3130 [D loss: 1.1760892868041992]  [G loss: 0.8517002463340759]\n",
      "3140 [D loss: 1.2145874500274658]  [G loss: 0.92524254322052]\n",
      "3150 [D loss: 1.3667620420455933]  [G loss: 0.8968947529792786]\n",
      "3160 [D loss: 1.2345147132873535]  [G loss: 0.8079319000244141]\n",
      "3170 [D loss: 1.2791829109191895]  [G loss: 0.831169605255127]\n",
      "3180 [D loss: 1.450637698173523]  [G loss: 0.868320643901825]\n",
      "3190 [D loss: 1.2132370471954346]  [G loss: 0.8989400267601013]\n",
      "3200 [D loss: 1.2595515251159668]  [G loss: 0.8827890157699585]\n",
      "3210 [D loss: 1.3503621816635132]  [G loss: 0.9039092063903809]\n",
      "3220 [D loss: 1.4345074892044067]  [G loss: 0.8462755680084229]\n",
      "3230 [D loss: 1.3176178932189941]  [G loss: 0.7854160070419312]\n",
      "3240 [D loss: 1.2355384826660156]  [G loss: 0.8555134534835815]\n",
      "3250 [D loss: 1.2075756788253784]  [G loss: 0.8506729602813721]\n",
      "3260 [D loss: 1.1157608032226562]  [G loss: 0.9533606767654419]\n",
      "3270 [D loss: 1.1624815464019775]  [G loss: 0.9377303719520569]\n",
      "3280 [D loss: 1.2963757514953613]  [G loss: 0.8818652629852295]\n",
      "3290 [D loss: 1.1788008213043213]  [G loss: 0.9011770486831665]\n",
      "3300 [D loss: 1.1551355123519897]  [G loss: 0.9278185367584229]\n",
      "3310 [D loss: 1.1971814632415771]  [G loss: 1.0668790340423584]\n",
      "3320 [D loss: 1.246762752532959]  [G loss: 0.8819898366928101]\n",
      "3330 [D loss: 1.414372205734253]  [G loss: 0.689767062664032]\n",
      "3340 [D loss: 1.3706302642822266]  [G loss: 0.7824429273605347]\n",
      "3350 [D loss: 1.2336935997009277]  [G loss: 0.8316447734832764]\n",
      "3360 [D loss: 1.2002944946289062]  [G loss: 0.9726216197013855]\n",
      "3370 [D loss: 1.259559154510498]  [G loss: 0.7617110013961792]\n",
      "3380 [D loss: 1.187687873840332]  [G loss: 0.9166653156280518]\n",
      "3390 [D loss: 1.477502703666687]  [G loss: 0.8285950422286987]\n",
      "3400 [D loss: 1.3584356307983398]  [G loss: 0.7946133017539978]\n",
      "3410 [D loss: 1.3690619468688965]  [G loss: 0.7708110809326172]\n",
      "3420 [D loss: 1.4604554176330566]  [G loss: 0.710197925567627]\n",
      "3430 [D loss: 1.3304712772369385]  [G loss: 0.8162016868591309]\n",
      "3440 [D loss: 1.3227784633636475]  [G loss: 0.8056731224060059]\n",
      "3450 [D loss: 1.0346715450286865]  [G loss: 0.9969131350517273]\n",
      "3460 [D loss: 0.9937152862548828]  [G loss: 0.9871522188186646]\n",
      "3470 [D loss: 1.2889683246612549]  [G loss: 0.9516251087188721]\n",
      "3480 [D loss: 1.4071457386016846]  [G loss: 0.9020347595214844]\n",
      "3490 [D loss: 1.453690767288208]  [G loss: 0.7983434200286865]\n",
      "3500 [D loss: 1.2398171424865723]  [G loss: 0.846573531627655]\n",
      "3510 [D loss: 1.3047866821289062]  [G loss: 0.8379188776016235]\n",
      "3520 [D loss: 1.2074971199035645]  [G loss: 0.8725953102111816]\n",
      "3530 [D loss: 1.2832074165344238]  [G loss: 0.8474549055099487]\n",
      "3540 [D loss: 1.1609328985214233]  [G loss: 0.7916096448898315]\n",
      "3550 [D loss: 1.0919930934906006]  [G loss: 0.9120521545410156]\n",
      "3560 [D loss: 1.1596070528030396]  [G loss: 0.9147425293922424]\n",
      "3570 [D loss: 1.2368381023406982]  [G loss: 1.018702745437622]\n",
      "3580 [D loss: 1.3732670545578003]  [G loss: 0.9385163187980652]\n",
      "3590 [D loss: 1.307126522064209]  [G loss: 0.8544253706932068]\n",
      "3600 [D loss: 1.2540576457977295]  [G loss: 0.9542981386184692]\n",
      "3610 [D loss: 1.1591955423355103]  [G loss: 0.9527897834777832]\n",
      "3620 [D loss: 1.2879343032836914]  [G loss: 1.1109638214111328]\n",
      "3630 [D loss: 1.4628651142120361]  [G loss: 0.9967330694198608]\n",
      "3640 [D loss: 1.4862148761749268]  [G loss: 0.7180041074752808]\n",
      "3650 [D loss: 1.2374765872955322]  [G loss: 0.8118971586227417]\n",
      "3660 [D loss: 1.022094488143921]  [G loss: 0.9462180733680725]\n",
      "3670 [D loss: 1.0526156425476074]  [G loss: 1.011806607246399]\n",
      "3680 [D loss: 1.0277875661849976]  [G loss: 0.9141699075698853]\n",
      "3690 [D loss: 1.1548397541046143]  [G loss: 0.835961103439331]\n",
      "3700 [D loss: 1.4248026609420776]  [G loss: 1.0197749137878418]\n",
      "3710 [D loss: 1.3662899732589722]  [G loss: 1.1033775806427002]\n",
      "3720 [D loss: 1.348402976989746]  [G loss: 0.9096022844314575]\n",
      "3730 [D loss: 1.26035737991333]  [G loss: 0.8811917901039124]\n",
      "3740 [D loss: 1.2166301012039185]  [G loss: 0.9111120700836182]\n",
      "3750 [D loss: 1.4150625467300415]  [G loss: 1.0339292287826538]\n",
      "3760 [D loss: 1.2516945600509644]  [G loss: 0.9460855722427368]\n",
      "3770 [D loss: 1.346614122390747]  [G loss: 0.8805574178695679]\n",
      "3780 [D loss: 1.053478479385376]  [G loss: 1.00315260887146]\n",
      "3790 [D loss: 0.9258909821510315]  [G loss: 1.0655418634414673]\n",
      "3800 [D loss: 1.0631524324417114]  [G loss: 0.9935970306396484]\n",
      "3810 [D loss: 1.158644437789917]  [G loss: 0.9302456378936768]\n",
      "3820 [D loss: 1.380725622177124]  [G loss: 0.883202075958252]\n",
      "3830 [D loss: 1.3325952291488647]  [G loss: 0.936348557472229]\n",
      "3840 [D loss: 1.3814420700073242]  [G loss: 0.8596904277801514]\n",
      "3850 [D loss: 1.295238971710205]  [G loss: 1.0016958713531494]\n",
      "3860 [D loss: 1.1317386627197266]  [G loss: 0.9550022482872009]\n",
      "3870 [D loss: 1.034932017326355]  [G loss: 1.057489275932312]\n",
      "3880 [D loss: 1.059088945388794]  [G loss: 1.1555724143981934]\n",
      "3890 [D loss: 1.1914910078048706]  [G loss: 1.016465187072754]\n",
      "3900 [D loss: 1.1554744243621826]  [G loss: 0.7966083884239197]\n",
      "3910 [D loss: 1.1627733707427979]  [G loss: 0.7242176532745361]\n",
      "3920 [D loss: 1.0681955814361572]  [G loss: 0.8437045812606812]\n",
      "3930 [D loss: 1.0745582580566406]  [G loss: 1.0307087898254395]\n",
      "3940 [D loss: 1.0390833616256714]  [G loss: 1.0529460906982422]\n",
      "3950 [D loss: 1.3468141555786133]  [G loss: 1.0708582401275635]\n",
      "3960 [D loss: 1.532458782196045]  [G loss: 0.9745187759399414]\n",
      "3970 [D loss: 1.41637122631073]  [G loss: 0.8530242443084717]\n",
      "3980 [D loss: 1.3672544956207275]  [G loss: 0.7886253595352173]\n",
      "3990 [D loss: 1.18062162399292]  [G loss: 0.8250395059585571]\n",
      "4000 [D loss: 1.3114089965820312]  [G loss: 0.9430617690086365]\n",
      "4010 [D loss: 1.1828581094741821]  [G loss: 0.8809288740158081]\n",
      "4020 [D loss: 0.9992396831512451]  [G loss: 1.0496187210083008]\n",
      "4030 [D loss: 0.9109765887260437]  [G loss: 1.0363174676895142]\n",
      "4040 [D loss: 1.0567868947982788]  [G loss: 1.1194911003112793]\n",
      "4050 [D loss: 1.0215133428573608]  [G loss: 1.040431261062622]\n",
      "4060 [D loss: 1.1492356061935425]  [G loss: 1.0941022634506226]\n",
      "4070 [D loss: 1.17624831199646]  [G loss: 1.1673107147216797]\n",
      "4080 [D loss: 1.3733725547790527]  [G loss: 1.0721930265426636]\n",
      "4090 [D loss: 1.4000272750854492]  [G loss: 0.9285138249397278]\n",
      "4100 [D loss: 1.4434531927108765]  [G loss: 0.8289726972579956]\n",
      "4110 [D loss: 1.4428366422653198]  [G loss: 0.7403204441070557]\n",
      "4120 [D loss: 1.4552631378173828]  [G loss: 0.6868074536323547]\n",
      "4130 [D loss: 1.3153064250946045]  [G loss: 0.7930529117584229]\n",
      "4140 [D loss: 1.4009491205215454]  [G loss: 0.7774186730384827]\n",
      "4150 [D loss: 1.5014840364456177]  [G loss: 0.8054184317588806]\n",
      "4160 [D loss: 1.412871241569519]  [G loss: 0.8554694652557373]\n",
      "4170 [D loss: 1.270180106163025]  [G loss: 0.9367920160293579]\n",
      "4180 [D loss: 1.2352203130722046]  [G loss: 0.9416670799255371]\n",
      "4190 [D loss: 1.0681113004684448]  [G loss: 0.9842172265052795]\n",
      "4200 [D loss: 1.0723686218261719]  [G loss: 1.0145901441574097]\n",
      "4210 [D loss: 1.309471845626831]  [G loss: 0.7643415927886963]\n",
      "4220 [D loss: 1.1981616020202637]  [G loss: 0.8191522359848022]\n",
      "4230 [D loss: 1.3673279285430908]  [G loss: 0.7893728017807007]\n",
      "4240 [D loss: 1.3326829671859741]  [G loss: 0.7306548357009888]\n",
      "4250 [D loss: 1.3608744144439697]  [G loss: 0.8706940412521362]\n",
      "4260 [D loss: 1.4757516384124756]  [G loss: 0.8147948980331421]\n",
      "4270 [D loss: 1.4140961170196533]  [G loss: 0.843816339969635]\n",
      "4280 [D loss: 1.16836416721344]  [G loss: 0.885450005531311]\n",
      "4290 [D loss: 1.0094228982925415]  [G loss: 1.0139241218566895]\n",
      "4300 [D loss: 1.101272702217102]  [G loss: 1.035815954208374]\n",
      "4310 [D loss: 1.1077549457550049]  [G loss: 0.975544810295105]\n",
      "4320 [D loss: 1.2114512920379639]  [G loss: 0.810356855392456]\n",
      "4330 [D loss: 1.227033019065857]  [G loss: 0.7558762431144714]\n",
      "4340 [D loss: 1.0626473426818848]  [G loss: 0.9152977466583252]\n",
      "4350 [D loss: 1.1538431644439697]  [G loss: 0.9364527463912964]\n",
      "4360 [D loss: 1.239888072013855]  [G loss: 0.9510564804077148]\n",
      "4370 [D loss: 1.2559635639190674]  [G loss: 0.9865576028823853]\n",
      "4380 [D loss: 1.3740589618682861]  [G loss: 0.8371281623840332]\n",
      "4390 [D loss: 1.4929673671722412]  [G loss: 0.8154552578926086]\n",
      "4400 [D loss: 1.2321730852127075]  [G loss: 0.9546746015548706]\n",
      "4410 [D loss: 1.0720045566558838]  [G loss: 1.0162934064865112]\n",
      "4420 [D loss: 1.178992509841919]  [G loss: 1.0831049680709839]\n",
      "4430 [D loss: 1.3889622688293457]  [G loss: 0.9917682409286499]\n",
      "4440 [D loss: 1.3334581851959229]  [G loss: 0.8465616703033447]\n",
      "4450 [D loss: 1.2901484966278076]  [G loss: 0.7622008919715881]\n",
      "4460 [D loss: 1.071507215499878]  [G loss: 0.9086165428161621]\n",
      "4470 [D loss: 0.9897024631500244]  [G loss: 1.081680417060852]\n",
      "4480 [D loss: 1.2122514247894287]  [G loss: 1.034026861190796]\n",
      "4490 [D loss: 1.425287127494812]  [G loss: 0.812880277633667]\n",
      "4500 [D loss: 1.488612413406372]  [G loss: 0.7251707315444946]\n",
      "4510 [D loss: 1.3836665153503418]  [G loss: 0.9929123520851135]\n",
      "4520 [D loss: 1.2310391664505005]  [G loss: 1.031972885131836]\n",
      "4530 [D loss: 1.2317090034484863]  [G loss: 0.8370801210403442]\n",
      "4540 [D loss: 1.4355806112289429]  [G loss: 0.6472975611686707]\n",
      "4550 [D loss: 1.4376578330993652]  [G loss: 0.7142002582550049]\n",
      "4560 [D loss: 1.3362481594085693]  [G loss: 0.7392417192459106]\n",
      "4570 [D loss: 1.1982052326202393]  [G loss: 0.8598535060882568]\n",
      "4580 [D loss: 1.0366601943969727]  [G loss: 0.9017485976219177]\n",
      "4590 [D loss: 1.2064199447631836]  [G loss: 0.9168245792388916]\n",
      "4600 [D loss: 1.3843927383422852]  [G loss: 0.8306912183761597]\n",
      "4610 [D loss: 1.338365912437439]  [G loss: 0.9539591073989868]\n",
      "4620 [D loss: 1.4130316972732544]  [G loss: 0.9866511821746826]\n",
      "4630 [D loss: 0.9976489543914795]  [G loss: 1.1123876571655273]\n",
      "4640 [D loss: 0.8662422895431519]  [G loss: 1.1633753776550293]\n",
      "4650 [D loss: 0.9367779493331909]  [G loss: 1.3442093133926392]\n",
      "4660 [D loss: 1.173980951309204]  [G loss: 1.0427719354629517]\n",
      "4670 [D loss: 1.4417166709899902]  [G loss: 0.6837299466133118]\n",
      "4680 [D loss: 1.628859043121338]  [G loss: 0.681255578994751]\n",
      "4690 [D loss: 1.1894794702529907]  [G loss: 0.9194233417510986]\n",
      "4700 [D loss: 0.9524708986282349]  [G loss: 1.1793677806854248]\n",
      "4710 [D loss: 0.9680456519126892]  [G loss: 1.0723237991333008]\n",
      "4720 [D loss: 1.40012526512146]  [G loss: 0.912026047706604]\n",
      "4730 [D loss: 1.613631010055542]  [G loss: 0.7576464414596558]\n",
      "4740 [D loss: 1.5632288455963135]  [G loss: 0.8436277508735657]\n",
      "4750 [D loss: 1.2918269634246826]  [G loss: 0.989127516746521]\n",
      "4760 [D loss: 1.2220098972320557]  [G loss: 0.8909271955490112]\n",
      "4770 [D loss: 1.3974348306655884]  [G loss: 0.9097017645835876]\n",
      "4780 [D loss: 1.2658445835113525]  [G loss: 0.8102622032165527]\n",
      "4790 [D loss: 1.2151272296905518]  [G loss: 0.7994969487190247]\n",
      "4800 [D loss: 1.2554948329925537]  [G loss: 0.8885301351547241]\n",
      "4810 [D loss: 1.193337321281433]  [G loss: 0.9285590052604675]\n",
      "4820 [D loss: 1.1198439598083496]  [G loss: 1.021485447883606]\n",
      "4830 [D loss: 1.1119412183761597]  [G loss: 0.9569306373596191]\n",
      "4840 [D loss: 1.2166088819503784]  [G loss: 0.8909825086593628]\n",
      "4850 [D loss: 1.3500233888626099]  [G loss: 0.9368863701820374]\n",
      "4860 [D loss: 1.3597888946533203]  [G loss: 0.7490347623825073]\n",
      "4870 [D loss: 1.4027456045150757]  [G loss: 0.9049923419952393]\n",
      "4880 [D loss: 1.2763303518295288]  [G loss: 0.8772344589233398]\n",
      "4890 [D loss: 1.1251556873321533]  [G loss: 0.9322671890258789]\n",
      "4900 [D loss: 1.0622607469558716]  [G loss: 0.9419772624969482]\n",
      "4910 [D loss: 1.1723307371139526]  [G loss: 0.8878071308135986]\n",
      "4920 [D loss: 1.2114810943603516]  [G loss: 0.96064293384552]\n",
      "4930 [D loss: 1.2324680089950562]  [G loss: 0.7747993469238281]\n",
      "4940 [D loss: 1.310630440711975]  [G loss: 0.7548078298568726]\n",
      "4950 [D loss: 1.3429803848266602]  [G loss: 0.8937962055206299]\n",
      "4960 [D loss: 1.3796932697296143]  [G loss: 0.8685921430587769]\n",
      "4970 [D loss: 1.5044050216674805]  [G loss: 0.7853163480758667]\n",
      "4980 [D loss: 1.435232400894165]  [G loss: 0.8703641891479492]\n",
      "4990 [D loss: 1.2712786197662354]  [G loss: 0.9280668497085571]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Training parameters \n",
    "\n",
    "batch_size = 64\n",
    "#batch_size = 258   #google batch_size\n",
    "epochs = 5000\n",
    "sample_interval = 10\n",
    "\n",
    "# Adversarial ground truths \n",
    "real = np.ones((batch_size, 1)) \n",
    "fake = np.zeros((batch_size, 1)) \n",
    "\n",
    "# Training loop \n",
    "for epoch in range(epochs): \n",
    "    # Train the discriminator \n",
    "    #for _ in range(x_train.shape[0]//batch_size):#sample everything on average per epoch, this is what google's tutorial does\n",
    "        idx = np.random.randint(0, x_train.shape[0], batch_size) #hmm so main diff with google code is that this seems to be outside for loop and do all batches**\n",
    "        real_images = x_train[idx] \n",
    "        noise = np.random.normal(0, 1, (batch_size, 100)) \n",
    "    \n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            generated_images = generator(noise, training=True) \n",
    "\n",
    "            real_output = discriminator(real_images, training=True)\n",
    "            fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "            gen_loss = generator_loss(fake_output)\n",
    "            disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    \n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "        # Print the progress \n",
    "        if epoch % sample_interval == 0: \n",
    "            print(f\"{epoch} [D loss: {disc_loss}]  [G loss: {gen_loss}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Assessing the Quality of Generated Images \n",
    "\n",
    "### Objective: \n",
    "- Evaluate the performance of the trained GAN. \n",
    "\n",
    "### 1. Qualitative Assessment: Visual Inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m137.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m147.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m135.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pillow-11.1.0 pyparsing-3.2.1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def sample_images(generator, epoch, num_images=25): \n",
    "    noise = np.random.normal(0, 1, (num_images, 100)) \n",
    "    generated_images = generator.predict(noise) \n",
    "    generated_images = 0.5 * generated_images + 0.5  # Rescale to [0, 1] \n",
    "    fig, axs = plt.subplots(5, 5, figsize=(10, 10)) \n",
    "    count = 0 \n",
    "\n",
    "    for i in range(5): \n",
    "        for j in range(5): \n",
    "            axs[i, j].imshow(generated_images[count, :, :, 0], cmap='gray') \n",
    "            axs[i, j].axis('off') \n",
    "            count += 1 \n",
    "    plt.show() \n",
    "\n",
    "# Sample images at the end of training \n",
    "sample_images(generator, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Quantitative Assessment: Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "Discriminator Accuracy on Real Images: 53.12%\n",
      "Discriminator Accuracy on Fake Images: 89.06%\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the discriminator accuracy on real vs. fake images\n",
    "noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "generated_images = generator.predict(noise)\n",
    "\n",
    "# Evaluate the discriminator on real images\n",
    "real_images = x_train[np.random.randint(0, x_train.shape[0], batch_size)]\n",
    "d_loss_real = discriminator.evaluate(real_images, np.ones((batch_size, 1)), verbose=0)\n",
    "\n",
    "# Evaluate the discriminator on fake images\n",
    "d_loss_fake = discriminator.evaluate(generated_images, np.zeros((batch_size, 1)), verbose=0)\n",
    "\n",
    "print(f\"Discriminator Accuracy on Real Images: {d_loss_real[1] * 100:.2f}%\")\n",
    "print(f\"Discriminator Accuracy on Fake Images: {d_loss_fake[1] * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of results \n",
    "\n",
    "### - Images not as blurry as DNN-based GAN architecture in try6.\n",
    "- pixels of the same color aggregate more easily\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: despite using the exact same generator and discriminator as in [Google's GAN tutorial](https://www.tensorflow.org/tutorials/generative/dcgan), results do not look as good because they train their model for longer. They train for 50 epochs but each epoch goes through all MNIST images with a batch size of 258. For my code to have similiar training with the same batch size, I would need to run my code for 60000/258 * 50 > 11,000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "a15aba3b24bff4b757cc330b887b99e24759a4bc72375ddd66d988acbcabf860"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
